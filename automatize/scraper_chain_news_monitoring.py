# -*- coding: utf-8 -*-
"""Scraper_Chain_News_Monitoring.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1yEX29WvDBgNjj_zIcLlGxvAg5zBN0FoH
"""

from gettext import npgettext
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import re
import feedparser
import socket
from datetime import datetime
import pickle
import helper
import numpy as np
from sklearn.metrics import precision_score
from google.cloud import translate_v2 as translate
from langdetect import detect
import os
import logging
import warnings
from GoogleNews import GoogleNews
import dateutil.parser
import pandas as pd
import glob
from deep_translator import (GoogleTranslator,
                             MyMemoryTranslator,
                             QcriTranslator)
from libretranslatepy import LibreTranslateAPI

google_translator = GoogleTranslator(target='en')
my_memory_translator = MyMemoryTranslator(target="en")
qcri_translator = QcriTranslator("88d615c8b6c44c298a86ae986628cb1d",target="en")
libre_translator = LibreTranslateAPI("https://translate.argosopentech.com/")
libre_translator_supported_languages=list(pd.DataFrame(libre_translator.languages())["code"])



warnings.filterwarnings("ignore")
logging.basicConfig(filename="scraper.log",
                        format="%(asctime)s %(message)s",
                        encoding="utf-8",
                        level=logging.INFO)


headers = {
    'user-agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36"}

"""Get actual Date"""

#actual_date = datetime.now().strftime("%d" + "." + "%m" + "." + "%Y")


""" Detect and Translate Text to English """
def detect_english(text):
    try:
        if detect(text) != 'en':
            return False
        else:
            return True
    except:
         return True

def translate_text(text, target= 'en'):
    TRANSLATE_API_PATH = ""
    translate_client = translate.Client.from_service_account_json(TRANSLATE_API_PATH)
    result = translate_client.translate(text, target_language=target)
    # out = {}
    # out["translatedText"] = result["translatedText"]
    # out["detectedSourceLanguage"] = result["detectedSourceLanguage"]
    return result["translatedText"]


""" Detect and Translate Text to English - newer version based on ensemble of translators """
def translate_text_2(text):
  source=detect(text)
  try:
    if source in google_translator.get_supported_languages(as_dict=True).values():
      google_translator.source="auto"
      translated=google_translator.translate(text=text)
      return translated
  except:
    pass
  try:  
    if source in my_memory_translator.get_supported_languages(as_dict=True).values():
      my_memory_translator.source=source
      translated=my_memory_translator.translate(text=text)
      return translated
  except:
    pass
  try:
    if source in qcri_translator.get_supported_languages(as_dict=True).values():
      qcri_translator.source=source
      translated=qcri_translator.translate(text=text)
      logging.info(" Used Qcri Translator: Language Detected  %s  ",source)
      return translated
  except:
    pass

  try:
    if source in libre_translator_supported_languages:
      translated=libre_translator.translate(q=text,source=source,target='en')
      logging.info(" Used Libre Translator: Language Detected  %s  ",source)
      return translated
  except:
    pass
  return ""


"""Parsing the "description" of the item"""

def parse_item(description):
    d_soup = BeautifulSoup(description.text, 'html.parser')
    try:
        item = d_soup.text
        if len(item.split()) < 2:
            item = ""
    except:
        try:
            item = d_soup.find('img')['title']
        except:
            item = ""
    return item


"""Verify the match between tags of RSS-Feeds and general storage header name"""

def verifyMatch(match, text):
    if re.search(str(match), text.lower()):
        return True
    else:
        return False

"""Function to check if Url is available in on synonymn fields"""
def verifyMatchURL(text):
    synonymURL = ['url', 'link', 'source']
    for tag in text:
        for synonym in synonymURL:
            if re.search(synonym.lower(), tag.lower()):
                return tag

    return None

"""Function to check if Date is available in on synonymn fields"""
def verifyMatchDate(text):
    synonymDate = ['date', 'published']
    for tag in text:
        for synonym in synonymDate:
            if re.search(synonym.lower(), tag.lower()):
                return tag

    return None


"""Find tags name of the RSS-Feeds"""
def findAllTags(soup):
    listItem = []
    for tag in soup.find_all("item"):
        listItem.append(tag)

    return listItem


def getAllTags(item):
    return [tag.name for tag in item.find_all()]


"""Function to convert Date to Unix"""
def date_converter_to_unix(ts):
  try:
    return datetime.strptime(ts,"%Y-%m-%d").timestamp()*1000
  except:
    print("time stamp empty")
    return datetime.strptime("1970-01-01","%Y-%m-%d").timestamp()*1000


pca_file_name ="automatize/pca_2nd_version.pkl"
with open(pca_file_name, 'rb') as file:
    pca = pickle.load(file)

model_file_name ="automatize/SGDClassifier_2nd_version.pkl"
with open(model_file_name, 'rb') as file:
    model = pickle.load(file)


""" function for predicting label"""
def prediction(title, content):
    global model
    global pca
    try:
        content = helper.preprocessing(content)
        doc_Embedding = helper.document_vector(content)
        doc_Embedding2 = list(doc_Embedding) + \
            list(pca.transform(doc_Embedding.reshape(1, -1))[0])
        label = model.predict(np.array(doc_Embedding2).reshape(1, -1))
        return label[0]
    except:
        try:
            title = helper.preprocessing(title)
            doc_Embedding = helper.document_vector(title)
            doc_Embedding2 = list(
                doc_Embedding)+list(pca.transform(doc_Embedding.reshape(1, -1))[0])
            label = model.predict(np.array(doc_Embedding2).reshape(1, -1))
            return label[0]
        except:
            return 0


"""Extract/Parse the items values of the RSS-Feeds in form of a dictionary"""
def get_list_items(list_item, x, tags, rss_link, latest_ids):
    items = {}
    synonym_url = verifyMatchURL(tags)
    synonym_date = verifyMatchDate(tags)
    translated_boolean=False

    for count, item in enumerate(tqdm(list_item)):
        actualItem = {}

        # parsing date
        try:
            actualItem["ts"] = x.entries[count].published
        except:
            if synonym_date != None:
                actualItem["ts"] = item(synonym_date)[0].text
        actualItem["ts"] = str(helper.parse_date( actualItem["ts"]))
        actualItem["ts"] = date_converter_to_unix(actualItem["ts"])

        actual_date = datetime.now().strftime("%Y" + "-" + "%m" + "-" + "%d")
        actual_date=datetime.strptime(actual_date,"%Y-%m-%d").timestamp()

        # parsing url
        try:
            actualItem["source"] = x.entries[count].link
        except:
            if synonym_url != None:
                actualItem["source"] = item(synonym_url)[0].text
        
        #getting id of article
        try:
            id=item("id")[0].text
        except:
            id=actualItem["source"]
        
        #skipping already saved articles
        if id in latest_ids:
            continue
        
        #skipping articles older than 5 days
        #date_diff=(float(actual_date)-float(actualItem["ts"]/1000))/(60*60*24)
        #if date_diff>5:
        #    continue

        #parsing content
        try:
            parsed_content = parse_item(item("description")[0])
            if detect_english(parsed_content) == False:
                actualItem["content"] = translate_text_2(parsed_content)
                if translated_boolean==False:
                    logging.info(" Translated language for -  %s ",rss_link)
                    print(" Translated language for -  %s ",rss_link)
                    translated_boolean=True
            else:
                actualItem["content"] = parsed_content
        except:
            actualItem["content"] = ""
        actualItem["content"] = actualItem["content"].replace('"', r'\"')



        # parsing title
        parsed_title = x.entries[count].title
        if detect_english(parsed_title) == False:
            actualItem["title"] = translate_text_2(parsed_title)
        else:
            actualItem["title"] = parsed_title
        actualItem["title"] = actualItem["title"].replace('"', r'\"')

        # append class label
        if len(actualItem["content"].split())>5:
            actualItem["label"] = prediction(actualItem["title"], actualItem["content"])
        else:
            actualItem["label"] = prediction(actualItem["title"],"")

        # parsing RSS Link
        actualItem["RSS"] = rss_link


        try:
            # Do not include articles without content and title
            if actualItem["content"]=="" and actualItem["title"]=="":
                pass
            else:
                items[item("id")[0].text] = actualItem
        except:
            if actualItem["content"]=="" and actualItem["title"]=="":
                pass
            else:
                items[actualItem["source"]] = actualItem
    return items
    
"""Reading the links of RSS-Feeds"""
def scraper_RSS_Feeds_Dict(links):
    folder_path=r"data"
    latest_ids=get_latest_article_ids(folder_path)
    # creating a dict of dictionaries
    dict_of_dict = {}
    for link in tqdm(links):
        print(link)

        # try: parsing the given link
        # except: skip the list after 60.0 s
        socket.setdefaulttimeout(60.0)
        try:
            r = requests.get(link, headers=headers, timeout = 60)
            # print(r.text)
            y = feedparser.parse(link)
            #solving encoding issues
            html = r.content.decode(r.apparent_encoding)
            #soup = BeautifulSoup(r.text, "html.parser")
            soup = BeautifulSoup(html, "html.parser")
            
            logging.info("%s - link  worked", link)
        except:
            logging.info("%s - link did not work", link)
            pass

        # verify if the status code of the page is "ok"
        if r.status_code == 200:
            try:
                tags = getAllTags(soup.find("item"))
                allItems = get_list_items(soup.find_all("item"), y, tags, link,latest_ids)
                dict_of_dict = {**dict_of_dict, **allItems}
            except:
                logging.info("%s worked but error in parsing", link)
                pass
        else:
            pass

    return dict_of_dict

"""function to convert dict_of_dicts to list_of_dicts"""
def scraper_RSS_Feeds_List(dict_of_dicts):
    list_of_dicts = []
    for i, j in tqdm(dict_of_dicts.items()):
        k = j
        k["id"] = i
        list_of_dicts.append(j)

    return list_of_dicts


"""Upload and download File"""


def upload_file(path, file):
    with open(path, 'wb') as f:
        pickle.dump(file, f)


def download_file(path):
    with open(path, 'rb') as f:
        loaded_dict = pickle.load(f)
    return loaded_dict
    

"""get ids of articles scraped in the last 5 days"""
def get_latest_article_ids(folder_path):
    list_of_files = glob.glob(folder_path+"/list_of_dictionary_*")
    last_n_modified_files=[]
    last_n_days_articles=[]
    for i in range(0,5):
        latest_file = max(list_of_files, key=os.path.getctime)
        last_n_modified_files.append(latest_file)
        last_n_days_articles=last_n_days_articles+download_file(latest_file)
        list_of_files=list(set(list_of_files)-set([latest_file]))
    last_n_days_articles=pd.DataFrame(last_n_days_articles)
    return list(last_n_days_articles["id"])



"""Extract/Parse the items values of the Google-News Feeds in form of a dictionary"""
def scraperGoogleNews():
    print("Starting google news scraping")

    googlenews = GoogleNews()
    queries=pd.read_csv(r"automatize/keywords.csv",header=None)
    queries=list(queries[0])
    folder_path=r"data"
    latest_ids=get_latest_article_ids(folder_path)
    list_of_dicts=[]
    for query in queries:
        print(query)
        googlenews.get_news(query)
        result = googlenews.results()
        #return result
        rss_feed_link="https://news.google.com/rss/search?q="+query.replace(" ","+")+"&hl=eN"
        for article in tqdm(result):
            actualItem = {}
            #handling the extra "." in the link
            #"news.google.com/./articles/CBMi" is converted to
            #"news.google.com/articles/CBMi"
            try:
                if article["link"].split("news.google.com/")[1][0]==".":
                    article["link"] = 'news.google.com' + article["link"].split("news.google.com/")[1][1:]
                
                actualItem["content"] = ""

                # parsing date
                actualItem["ts"] = str(article["datetime"].date())
                actualItem["ts"] = date_converter_to_unix(actualItem["ts"])

                #getting today's date into unix format
                actual_date = datetime.now().strftime("%Y" + "-" + "%m" + "-" + "%d")
                actual_date=datetime.strptime(actual_date,"%Y-%m-%d").timestamp()

                #skipping already saved articles
                if article["link"] in latest_ids:
                    continue
        
                #skipping articles older than 5 days
                #date_diff=(float(actual_date)-float(actualItem["ts"]/1000))/(60*60*24)
                #if date_diff>5:
                #    continue

                # parsing title
                parsed_title = article["title"]
                if detect_english(parsed_title) == False:
                    actualItem["title"] = translate_text_2(parsed_title)
                else:
                    actualItem["title"] = parsed_title
                actualItem["title"] = actualItem["title"].replace('"', r'\"')

                # append class label
                actualItem["label"] = prediction(actualItem["title"], actualItem["content"])

                actualItem["source"] =article["link"]

                # parsing RSS Link
                #actualItem["RSS"] = article["media"]
                actualItem["RSS"] = rss_feed_link



                # Do not include articles without content and title
                if actualItem["content"]=="" and actualItem["title"]=="":
                    continue
                else:
                    actualItem["id"]= article["link"]
                list_of_dicts.append(actualItem)
            except:
                pass
        # print(len(result))
        # print(type(result))
        # break
    return(list_of_dicts)

"""Main function to start scraping process everyday- Called in automation.py"""
def scraper():
    SAVE_PATH = r"data"
    actual_date = datetime.now().strftime("%d" + "." + "%m" + "." + "%Y")
    PATH_DICTIONARY = SAVE_PATH + "/dictionary_of_dictionary_" + actual_date
    PATH_LIST = SAVE_PATH + "/list_of_dictionary_" + actual_date + ".pkl"


    logging.info(str(actual_date))
    logging.info("Scraper has started.")

    with open("automatize/links.pkl", "rb") as jfile:
      links = pickle.load(jfile)
    print(f"Scraping total {len(links)} no of rss feeds")
   
    logging.info(str(f"Scraping total {len(links)}  no of rss feeds"))
    listPath = list()
    j = 0
    for i in range(1000, len(links), 1000):
        links1 = links[j: i]
        dict_of_dicts = scraper_RSS_Feeds_Dict(links1)
        upload_file(PATH_DICTIONARY + str(j) + "_" + str(i) + ".pkl", dict_of_dicts)
        listPath.append(PATH_DICTIONARY + str(j) + "_" + str(i) + ".pkl")
        j = i
    try:
      links1 = links[j: len(links)]
      dict_of_dicts = scraper_RSS_Feeds_Dict(links1)
      upload_file(PATH_DICTIONARY + str(j) + "_" + str(len(links)) + ".pkl", dict_of_dicts)
      listPath.append(PATH_DICTIONARY + str(j) + "_" + str(len(links)) + ".pkl")
    except:
    	pass
    
    dict_of_dicts = dict()
    for path in listPath:
        actualPath = download_file(path)
        dict_of_dicts = {**dict_of_dicts, **actualPath}

    
    list_of_dicts = scraper_RSS_Feeds_List(dict_of_dicts)
    google_list=scraperGoogleNews()
    list_of_dicts  = list_of_dicts+google_list
    print(PATH_LIST)


    upload_file(PATH_LIST, list_of_dicts)
